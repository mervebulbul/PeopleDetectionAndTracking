{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0110ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mrvbu\\anaconda3\\envs\\myenv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using cache found in C:\\Users\\mrvbu/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "c:\\Users\\mrvbu\\anaconda3\\envs\\myenv\\lib\\site-packages\\torchvision\\models\\detection\\anchor_utils.py:63: UserWarning: Failed to initialize NumPy: module compiled against API version 0x10 but this version of numpy is 0xf (Triggered internally at ..\\torch\\csrc\\utils\\tensor_numpy.cpp:77.)\n",
      "  device: torch.device = torch.device(\"cpu\"),\n",
      "YOLOv5  2023-2-2 Python-3.10.4 torch-1.13.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5n summary: 213 layers, 1867405 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Numpy is not available",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\mrvbu\\Untitled Folder 2\\Untitled17.ipynb Cell 1\u001b[0m in \u001b[0;36m<cell line: 91>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mrvbu/Untitled%20Folder%202/Untitled17.ipynb#W0sZmlsZQ%3D%3D?line=97'>98</a>\u001b[0m frame\u001b[39m=\u001b[39mcv2\u001b[39m.\u001b[39mresize(frameOrig, dsize)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mrvbu/Untitled%20Folder%202/Untitled17.ipynb#W0sZmlsZQ%3D%3D?line=98'>99</a>\u001b[0m displayGrid(frame)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/mrvbu/Untitled%20Folder%202/Untitled17.ipynb#W0sZmlsZQ%3D%3D?line=100'>101</a>\u001b[0m results \u001b[39m=\u001b[39m model(frame)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/mrvbu/Untitled%20Folder%202/Untitled17.ipynb#W0sZmlsZQ%3D%3D?line=102'>103</a>\u001b[0m persons \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mnames[\u001b[39m0\u001b[39m] \n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/mrvbu/Untitled%20Folder%202/Untitled17.ipynb#W0sZmlsZQ%3D%3D?line=104'>105</a>\u001b[0m person_counter \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\mrvbu\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\mrvbu\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:700\u001b[0m, in \u001b[0;36mAutoShape.forward\u001b[1;34m(self, ims, size, augment, profile)\u001b[0m\n\u001b[0;32m    698\u001b[0m     x \u001b[39m=\u001b[39m [letterbox(im, shape1, auto\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m im \u001b[39min\u001b[39;00m ims]  \u001b[39m# pad\u001b[39;00m\n\u001b[0;32m    699\u001b[0m     x \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mascontiguousarray(np\u001b[39m.\u001b[39marray(x)\u001b[39m.\u001b[39mtranspose((\u001b[39m0\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)))  \u001b[39m# stack and BHWC to BCHW\u001b[39;00m\n\u001b[1;32m--> 700\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mfrom_numpy(x)\u001b[39m.\u001b[39mto(p\u001b[39m.\u001b[39mdevice)\u001b[39m.\u001b[39mtype_as(p) \u001b[39m/\u001b[39m \u001b[39m255\u001b[39m  \u001b[39m# uint8 to fp16/32\u001b[39;00m\n\u001b[0;32m    702\u001b[0m \u001b[39mwith\u001b[39;00m amp\u001b[39m.\u001b[39mautocast(autocast):\n\u001b[0;32m    703\u001b[0m     \u001b[39m# Inference\u001b[39;00m\n\u001b[0;32m    704\u001b[0m     \u001b[39mwith\u001b[39;00m dt[\u001b[39m1\u001b[39m]:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Numpy is not available"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "#from pathlib import Path\n",
    "import time\n",
    "\n",
    "grid_size = 6\n",
    "grid_center = int(grid_size/2)\n",
    "grid_spacing = 50 # adjust as needed\n",
    "\n",
    "\n",
    "\n",
    "# Camera Settings\n",
    "camera_Width  = 1024 # 1280 # 640\n",
    "camera_Heigth = 780  # 960  # 480\n",
    "centerZone    = 100\n",
    "\n",
    "# GridLine color green and thickness\n",
    "lineColor = (0, 255, 0) \n",
    "lineThickness = 1\n",
    "\n",
    "# message color and thickness\n",
    "colorWhite = (255,255,255)\n",
    "colorBlack = (0,0,0)\n",
    "colorBlue = (255, 0, 0) \n",
    "colorGreen = (0, 255, 0) \n",
    "colorRed = (0, 0, 255) #red\n",
    "messageThickness = 2\n",
    "\n",
    "dsize = (camera_Width, camera_Heigth)\n",
    "\n",
    "def displayGrid(frame):\n",
    "    # Add a 5x5 Grid\n",
    "\n",
    "    for i in range(grid_size):\n",
    "        cv2.line(frame, (int(camera_Width/grid_size)*i, 0), (int(camera_Width/grid_size)*i, camera_Heigth), lineColor, lineThickness)\n",
    "        cv2.line(frame, (0, int(camera_Heigth/grid_size)*i), (camera_Width, int(camera_Heigth/grid_size)*i), lineColor, lineThickness)\n",
    "    \n",
    "    # Add center lines\n",
    "    cv2.line(frame, (int(camera_Width/2), 0), (int(camera_Width/2), camera_Heigth), lineColor, lineThickness)\n",
    "    cv2.line(frame, (0, int(camera_Heigth/2)), (camera_Width, int(camera_Heigth/2)), lineColor, lineThickness)\n",
    "\n",
    "    \n",
    "def calculatePositionForDetectedPerson(frame, x, y, h, w):\n",
    "\n",
    "    # calculate direction and relative position of the person\n",
    "    cx = int(x + (w / 2))  # Center X of the person\n",
    "    cy = int(y + (h / 2))  # Center Y of the person\n",
    "    dir_list = []\n",
    "\n",
    "    if (cx < int(camera_Width/2)-grid_spacing*grid_center):\n",
    "        if (cx < int(camera_Width/2)-2*grid_spacing*grid_center):\n",
    "            dir_list.append(\"GO FAR LEFT \")\n",
    "        else:\n",
    "            dir_list.append(\"GO LEFT \")\n",
    "    elif (cx > int(camera_Width/2)+grid_spacing*grid_center):\n",
    "        if (cx > int(camera_Width/2)+2*grid_spacing*grid_center):\n",
    "            dir_list.append(\"GO FAR RIGHT \")\n",
    "        else:\n",
    "            dir_list.append(\"GO RIGHT \")\n",
    "        \n",
    "    if (cy < int(camera_Heigth/2)-grid_spacing*grid_center):\n",
    "        if (cy < int(camera_Heigth/2)-2*grid_spacing*grid_center):\n",
    "            dir_list.append(\"GO FAR UP \")\n",
    "        else:\n",
    "            dir_list.append(\"GO UP \")\n",
    "    elif (cy > int(camera_Heigth/2)+grid_spacing*grid_center):\n",
    "        if (cy > int(camera_Heigth/2)+2*grid_spacing*grid_center):\n",
    "            dir_list.append(\"GO FAR DOWN \")\n",
    "        else:\n",
    "            dir_list.append(\"GO DOWN \")\n",
    "            \n",
    "    if not dir_list:\n",
    "        cv2.putText(frame, \"CENTER\", (20, 50), cv2.FONT_HERSHEY_COMPLEX, 1, colorRed, 2)\n",
    "    else:\n",
    "        cv2.putText(frame, \" \".join(dir_list), (20, 50), cv2.FONT_HERSHEY_COMPLEX, 1, colorRed, 2)\n",
    "        \n",
    "    # display detected person frame, line from center and direction to go\n",
    "    cv2.line(frame, (int(camera_Width/2), int(camera_Heigth/2)), (cx, cy), colorWhite, messageThickness)\n",
    "    cv2.rectangle(frame, (x, y), (x + w, y + h), colorRed, messageThickness)\n",
    "    cv2.putText(frame, str(int(x)) + \" \" + str(int(y)), (x - 20, y - 45), cv2.FONT_HERSHEY_COMPLEX, 0.7, colorRed, messageThickness)\n",
    "\n",
    "\n",
    "\n",
    "video_capture=cv2.VideoCapture('lodosviddeo2.avi')\n",
    "time.sleep(1.0)\n",
    "\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5n', pretrained=True)\n",
    "\n",
    "\n",
    "while True:\n",
    "    \n",
    "    ret,frameOrig =video_capture.read()\n",
    "    if not ret:\n",
    "        break\n",
    "        \n",
    "\n",
    "    frame=cv2.resize(frameOrig, dsize)\n",
    "    displayGrid(frame)\n",
    "\n",
    "    results = model(frame)\n",
    "\n",
    "    persons = model.names[0] \n",
    "\n",
    "    person_counter = 0\n",
    "\n",
    "    person_id = 0\n",
    "   \n",
    "    df = results.pandas().xyxy[0]  # get the DataFrame of detected objects\n",
    "    person_boxes = {}  # dictionary to store the box with highest confidence for each person\n",
    "    for _, row in df.iterrows():\n",
    "        if row['name'] == persons:  # check if the detected object is a person\n",
    "            if 'person_id' not in row:  # if person_id is not assigned yet\n",
    "                row['person_id'] = person_counter  # assign a new person_id\n",
    "                person_counter += 1\n",
    "            else:\n",
    "                person_id = row['person_id']  # use the assigned person_id\n",
    "            if person_id not in person_boxes or row['confidence'] > person_boxes[person_id]['confidence']:\n",
    "                person_boxes[person_id] = row\n",
    "\n",
    "    # draw the box with highest confidence for each person\n",
    "    person_id = 0  # initialize person ID counter\n",
    "\n",
    "    for person_id, box in person_boxes.items():\n",
    "        x, y, w, h = int(box['xmin']), int(box['ymin']), int(box['xmax'] - box['xmin']), int(box['ymax'] - box['ymin'])\n",
    "        calculatePositionForDetectedPerson(frame, x, y, h, w)\n",
    "        cv2.putText(frame, f'Person {person_id}', (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 255), 2)\n",
    "        person_id += 1\n",
    "            #cv2.putText(frame, f'Persons: {total_persons}', (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "    cv2.imshow(\"Human Detection\",frame)\n",
    "    if cv2.waitKey(1)&0xFF == ord('q'):\n",
    "        break\n",
    "        \n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d97783c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4957879",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
